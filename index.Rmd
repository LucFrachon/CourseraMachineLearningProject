--- 
title: 'Practical Machine Learning Project: Human Activity Qualification'
author: "Luc Frachon"
date: "21 janvier 2016"
output: 
  html_document: 
    keep_md: yes
    fig_caption: yes
---

# Abstract

Using data collected through accelerometers worn by a group of people performing weight lifting, we fit classification algorithms to the data to predict whether the exercise was performed correcty or with different types of typical errors.  
After trying several models and parameters, we conclude that the best compromise between absolute performance and speed is provided by a Random Forest algorithm applied to data dimensionally reduced through PCA.

---


# 1. Set up and data preparation

This analysis was performed using the following set up: 

*  Computer: Intel i5-4440 with 8GB RAM, solid-state main hard drive
*  OS: Windows 10, build number 10586
*  R version 3.2.2
*  RStudio version 0.99.484
*  Locale = French_France.1252

In this analysis we use the following libraries: caret, ggplot2, dplyr, parallel and doParallel. These last two enable us to take advantage of the quad-core configuration to speed up calculations. We also set the randomiser seed to 2302.

```{r echo=FALSE, warning=FALSE, message=FALSE}
require(caret); require(ggplot2); require(dplyr); require(reshape2); require(rpart)

#Increase memory allocation and set up parallel computing:
memory.limit(16194)
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
fitControl <- trainControl(allowParallel = TRUE)
set.seed(2302)
```

As usual, we read the data and tidy it up to make it easier to use.
```{r echo=FALSE, warning=FALSE, message=FALSE}
rawData <- read.csv("data/pml-training.csv",
                    na.strings = c("", "NA", "#DIV/0!"))
# Some variables improperly loaded as factors or integer vectors:
colSelect <- select(rawData, -(X : cvtd_timestamp), -new_window,
                    -num_window, -classe)
for (c in names(colSelect)) {
    colSelect[, c] <- as.numeric(colSelect[, c])
}
# We get rid of the problematic and incomplete cvtd_timestamp variable
# and reconstruct our data set:
fullSet <- data.frame(select(rawData, (X : raw_timestamp_part_2),
                             new_window, num_window), colSelect, 
                      classe = as.factor(rawData$classe))
rm(rawData)
rm(colSelect)
```

The data comes is kindly made available from the team cited in References. It contains 19622 observations and 160 variables, 153 of which can be considered potential predictors. The outcome is 'classe', a categorical variable with 5 classes; A corresponds to correctly executed movement whereas B through E denote specific errors in execution.  
Exploratory analysis shows that the dataset is ordered by 'classe', then 'user_name', then 'num_window', then timestamps (*figure 1*). 

```{r echo= FALSE, fig.cap="*Figure 1: Structure of the raw dataset. Different colours indicate different human users.*", include=TRUE}
plot(fullSet$X, fullSet$num_window, 
     pch = as.character(fullSet$classe), 
     col = fullSet$user_name, xlab = "Row index", 
     ylab = "num_window")
```


The timestamps correspond to  2.5s intervals with a 0.5s overlap. Presumably, each window corresponds to one instance of weight lifting. We considered the possibility of grouping observations within each window and using aggregated statistics as predictors. Although this approach might have made training and prediction easier, we decided against it for three reasons:

* The research team had already tried different time aggregatesand decided that 2.5s gave the best results; it seemed somewhat superfluous to use a second layer of aggregation.
* In real life, there is no cue from the user to let the sensors know when they start and stop an activity, therefore relevant algorithms need to be able to predict without that information.
* We came up with excellent predictive performance without the need for this aggregation.

# 2. Pre-processing

The data contains many sparse columns; it is really an "all-or-nothing" situation: the majority of columns are empty or near-empty.
```{r echo = FALSE}
NAsByCol <- sapply(names(fullSet), 
                   function(c){sum(is.na(fullSet[, c])) /
                           length(fullSet[, c])})
```

```{r echo = FALSE, fig.cap="*Figure 2: Distribution of the proportion of NAs in each column*", fig.height=4 ,include = TRUE}
hist(NAsByCol, breaks = 50, col = "lightgrey", main = NULL)
```

We address this by setting a threshold at 50% and dropping any column containing more than NAs the threshold. This effectively drops all the NAs. There are no other zero- or near-zero variance predictors.

```{r}
thresh <- .50
fullSetNoNAs<- fullSet[ , colSums(is.na(fullSet)) <= 
                            thresh * length(fullSet[[1]])]
rm(fullSet)  # Housekeeping
# Check how the number of NAs remaining:
sum(is.na(fullSetNoNAs))
```

```{r}
# There are no other zero or near-zero variance predictors:
nearZeroVar(fullSetNoNAs[ , 7 : 58], freqCut = 95/5, saveMetrics = FALSE)
```

# 3. Data Partitioning

We have a large number of observations, therefore we can afford to partition the data in 3:

* Training set (60%)
* Cross-validation set (20%), which we will use to select and tune the best algorithm
* Test set (20%), on which to estimate the model's quality. A small sample of 20 test cases is also provided by Coursera, but that number is too small for a reliable assessment.

```{r}
trainIndex <- createDataPartition(fullSetNoNAs$classe, 
                                  p = 0.6, list = F)
train1 <- fullSetNoNAs[trainIndex, ]
testAndCv1 <- fullSetNoNAs[-trainIndex, ]
cvIndex <- createDataPartition(testAndCv1$classe, p = 0.5, list = F)
cv1 <- testAndCv1[cvIndex, ]
test1 <- testAndCv1[-cvIndex, ]
rm(fullSetNoNAs)  # Housekeeping
```

# 4. Model Fitting and Prediction
## 4.1. Algorithm selection
We tried several algorithms, starting with simple ones:

* Decision tree
* Linear Discriminant Analysis
* Naive Bayes
* Random Forest

The first three gave unsatisfactory prediction accuracy both on the training and cross-validation sets:  


Model        | Accuracy on CV set
------------ | -------------------
Tree         | 49.6%
LDA          | 70.6%
Naive Bayes  | 74.6%    

We will now focus on the Random Forest algorithm, which yielded the best results by far.

## 4.2. Random Forest on untransformed data
We first deploy the Random Forest algorithm on the training set without prior transformation, using the 'caret' package:

```{r message=FALSE, warning=FALSE, cache=TRUE, eval=TRUE}
forestFit <- train(classe ~ ., method = "rf", data = train1[, 7:59],
                   prox = T, trControl = fitControl)
```

For Random Forest, calculating accuracy by predicting on the training set is incorrect. A better measure of training accuracy is the OOB prediction error. Here, it is excellent as reported here:  

```{r cache = TRUE, echo = FALSE}
print(forestFit$finalModel)
```

Accuracy on the cross-validation set is also excellent:

```{r cache=TRUE}
forestCvPred <- predict(forestFit, newdata = cv1)
sum(forestCvPred == cv1[, 59]) / length(forestCvPred)
table(Actual = cv1[, 59], Prediction = forestCvPred)
```

The training time is quite long: `r round(forestFit$times$everything[3] / 60, 1)` minutes.  

However the more important measure for the designed application is prediction time, as the user expects near-instant feedback. From that perspective, we found that Naive-Bayes had the worst prediction time by far. Fortunately the Random Forest prediction time is quick (a few seconds).


## 4.3 Random Forest on Principal Components
An interesting question for datasets with a relatively large numbers of variables is how much accuracy we lose in relation to data compression levels. The method used here is Principal Component Analysis and we will assess accuracy over a range of different values for the threshold parameter (levels of variance explained by the principal components).

The RF algorithm includes bootstrapping which theoretically reduces the need for a cross-validation set. However we are going to evaluate the model against different threshold values, which means that to a certain extent, we will tune our parameters to the cross-validation set. We therefore need an untouched test set to evaluate our model's final performance.

We rely on the 'train' function to find the optimal number of tries for the Random Forest Algorithm, while we manually vary the PCA threshold (proportion of variance explained by the computed Principal Components)

```{r cache=TRUE, warning=FALSE}
PCA.thresh <- c(0.95, 0.9, 0.8, 0.7, 0.5)

rfResultsWithPCA <- function(t, outcome, model, trainData, cvData,
                              control) {
    results = list()
    outcomeIdx <- outcome != colnames(trainData)
    trainData.PCAselection <- trainData[ , outcomeIdx]
    cvData.PCAselection <- cvData[ , outcomeIdx]
    # Compute PCA with specified threshold:
    pcaModel <- preProcess(trainData.PCAselection, 
                           method = "pca", thresh = t)
    # Training set recomputed using PCs:
    trainPCA <- predict(pcaModel, trainData.PCAselection)
    # Train specified classification model using the PC dataset:
    modelFitPCA <- train(trainData[[outcome]] ~ . , method = model,
                         data = trainPCA, trControl = control)
    # Calculate predictions for the CV dataset based on those PCs:
    cvDataPCA <- predict(pcaModel, cvData.PCAselection)
    cvPredPCA <- predict(modelFitPCA, newdata = cvDataPCA)
    results$accuracy <- sum(cvPredPCA == 
                                cvData[[outcome]])/length(cvPredPCA)
    results$fitObj <- modelFitPCA
    results$pcaObj <- pcaModel

    return(results)
}

rfModels <- lapply(PCA.thresh, FUN = rfResultsWithPCA,
                        outcome = "classe",
                        model = "rf",
                        trainData = train1[, 7:59],
                        cvData = cv1[, 7:59],
                        control = fitControl)
```

```{r echo=FALSE, eval=FALSE}
save(forestFit, rfModels, file="variables.Rda")
```


We can plot accuracy vs running time:

```{r, warning=FALSE, message=FALSE}

accuracy <- numeric()
runtime <- numeric()

for (i in seq(1, length(PCA.thresh))){
    accuracy[i] <- rfModels[[i]]$accuracy
    runtime[i] <- rfModels[[i]]$fitObj$times$everything[3]
}
print(round(runtime))
print(round(accuracy, 3))

```

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.cap="*Figure 3: Accuacy vs. Runtime and PCA threshold values*"}
g <- qplot(x = runtime, y = accuracy, label = PCA.thresh)
g <- g + geom_point() + geom_line() + geom_text(nudge_y = -0.005)
g
```

It turns out that even at 50% variance retained in the PCA process, we already get 83% accuracy and the algorithm runs in 3.5 minutes. At 95% variance retained, the running time triples but accuracy is 97%. The sweet spot seems to be somewhere around 85-90% variance retained. In the remainder of this document, we will use a threshold of 90% but if we were very concerned with running times, 70% would be a perfectly viable choice.

#5. Performance assessment

Using our model including PCA (at 90% threshold) + Random Forest, we can run predictions on the test set, which we have not used so far:

```{r warning=FALSE, message=FALSE}
# Apply PCA to the test set:
testDataPCA <- predict(rfModels[[2]]$pcaObj, newdata = test1)
forestTestPred <- predict(rfModels[[2]]$fitObj, newdata = testDataPCA)
#Accuracy:
sum(forestTestPred == test1$classe) / nrow(test1)
table(Actual = test1$classe, Prediction = forestTestPred)
```

The prediction accuracy remains extremely satisfactory on the test set.

#6. Assignment quiz cases

The Coursera assignment includes 20 test cases that are used to assess the model performance.

```{r}
quizSetRaw <- read.csv("data/pml-testing.csv",
                    na.strings = c("", "NA", "#DIV/0!"))
# Some variables improperly loaded as factors or integer vectors:
colSelect <- select(quizSetRaw, -(X : cvtd_timestamp), -new_window,
                    -num_window)
for (c in names(colSelect)) {
    colSelect[, c] <- as.numeric(colSelect[, c])
}
# We get rid of the problematic and incomplete cvtd_timestamp variable
# and reconstruct our data set:
quizSet <- data.frame(select(quizSetRaw, (X : raw_timestamp_part_2),
                             new_window, num_window), colSelect)

# Apply PCA to the test set:
quizSetPCA <- predict(rfModels[[2]]$pcaObj, newdata = quizSet)
# Calculate predictions:
forestQuizPred <- predict(rfModels[[2]]$fitObj, newdata = quizSetPCA)
forestQuizPred
```

19 out of 20 predictions turned out to be correct (only #3 was incorrectly predicted as A).

Note: the "no-PCA" Random Forest algorithm finds all 20 cases correctly:
```{r}
forestQuizNoPCA <- predict(forestFit, newdata = quizSet)
forestQuizNoPCA
```

#Conclusions

This assignment demonstrated the predictive power of the Random Forest algorithm on high-dimensional data. The trade-off is fairly long computational times but this can be largely mitigated by operating dimensionality reduction during the pre-processing phase while retaining high levels of accuracy.

---

### References
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13).   Stuttgart, Germany: ACM SIGCHI, 2013.  
  
Read more: [http://groupware.les.inf.puc-rio.br/har#sbia_paper_section#ixzz3xsGogIbK]